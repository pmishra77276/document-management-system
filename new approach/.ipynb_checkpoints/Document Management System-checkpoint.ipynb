{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c041ea2-26b7-4305-b49a-5d2b60a1d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from doctr.models import ocr_predictor\n",
    "from doctr.io import DocumentFile\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import VisionEncoderDecoderModel, TrOCRProcessor,BitsAndBytesConfig\n",
    "from pdf2image import convert_from_path\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import json\n",
    "import csv\n",
    "import threading\n",
    "import requests\n",
    "# import response\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1500,\n",
    "        chunk_overlap=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76d8e8c-ad0f-4074-8221-24aafab3b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_extract_pdf(pdf_path, text_threshold=20):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    image_pages = 0\n",
    "    text_pages = 0\n",
    "    full_text = \"\"\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text()\n",
    "        images = page.get_images(full=True)\n",
    "\n",
    "        if len(text.strip()) < text_threshold and len(images) > 0:\n",
    "            image_pages += 1\n",
    "        else:\n",
    "            text_pages += 1\n",
    "            full_text += f\"\\n--- Page {page_num + 1} ---\\n{text}\"\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    if image_pages > text_pages:\n",
    "        return 0\n",
    "    else:\n",
    "        return full_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c44abd0-533a-4e08-a113-72141a657709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(pil_image):\n",
    "    \"\"\"Convert a PIL image to grayscale and apply adaptive thresholding.\"\"\"\n",
    "    img = np.array(pil_image)  \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 21, 10)\n",
    "    return Image.fromarray(binary).convert(\"RGB\")  \n",
    "def cluster_lines(boxes, y_threshold=15):\n",
    "    \"\"\"Groups bounding boxes into lines and sorts them correctly.\"\"\"\n",
    "    boxes = sorted(boxes, key=lambda b: b[1])\n",
    "    lines = []\n",
    "    current_line = []\n",
    "    for box in boxes:\n",
    "        if not current_line:\n",
    "            current_line.append(box)\n",
    "        else:\n",
    "            prev_box = current_line[-1]\n",
    "            if abs(box[1] - prev_box[1]) < y_threshold:  \n",
    "                current_line.append(box)\n",
    "            else:\n",
    "                lines.append(sorted(current_line, key=lambda b: b[0]))\n",
    "                current_line = [box]\n",
    "\n",
    "    if current_line:\n",
    "        lines.append(sorted(current_line, key=lambda b: b[0]))\n",
    "    return lines  \n",
    "def process_pdf(pdf_path,trocr_model,detector):\n",
    "    \"\"\"Extract text from a PDF, visualize detected text, and save results.\"\"\"\n",
    "    output_folder = \"extracted_data\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    pdf_name = os.path.basename(pdf_path)  \n",
    "    txt_filename = os.path.splitext(pdf_name)[0] + \".txt\"\n",
    "    txt_filepath = os.path.join(output_folder, txt_filename)\n",
    "\n",
    "    images = convert_from_path(pdf_path, dpi=300)  \n",
    "    extracted_text = []\n",
    "\n",
    "    for page_num, img in enumerate(images, start=1):\n",
    "        print(f\"Processing Page {page_num}...\")\n",
    "\n",
    "        temp_image_path = f\"temp_page_{page_num}.png\"\n",
    "        img.save(temp_image_path)\n",
    "\n",
    "        single_page_doc = DocumentFile.from_images(temp_image_path)\n",
    "        result = detector(single_page_doc)\n",
    "\n",
    "        img_width, img_height = img.size  \n",
    "\n",
    "        bounding_boxes = []\n",
    "        for page in result.pages:\n",
    "            for block in page.blocks:\n",
    "                for line in block.lines:\n",
    "                    (x_min, y_min), (x_max, y_max) = line.geometry\n",
    "                    left = int(x_min * img_width)\n",
    "                    top = int(y_min * img_height)\n",
    "                    right = int(x_max * img_width)\n",
    "                    bottom = int(y_max * img_height)\n",
    "                    bounding_boxes.append((left, top, right, bottom))\n",
    "\n",
    "        sorted_lines = cluster_lines(bounding_boxes)\n",
    "\n",
    "        cropped_lines, line_positions = [], []\n",
    "        for line_boxes in sorted_lines:\n",
    "            left = min(box[0] for box in line_boxes)\n",
    "            top = min(box[1] for box in line_boxes)\n",
    "            right = max(box[2] for box in line_boxes)\n",
    "            bottom = max(box[3] for box in line_boxes)\n",
    "\n",
    "            crop = img.crop((left, top, right, bottom))\n",
    "            processed_crop = preprocess_image(crop)\n",
    "            cropped_lines.append(processed_crop)\n",
    "            line_positions.append((left, top, right, bottom))\n",
    "\n",
    "        if cropped_lines:\n",
    "            batch_pixel_values = trocr_processor(images=cropped_lines, return_tensors=\"pt\").pixel_values.to(device)\n",
    "            with torch.no_grad():\n",
    "                generated_ids = trocr_model.generate(batch_pixel_values)\n",
    "                predictions = trocr_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        else:\n",
    "            predictions = []\n",
    "\n",
    "        extracted_text.append(f\"Page {page_num}:\\n\" + \"\\n\".join(predictions) + \"\\n\\n\")\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.imshow(img)\n",
    "\n",
    "        for (left, top, right, bottom), text in zip(line_positions, predictions):\n",
    "            rect = patches.Rectangle((left, top), right - left, bottom - top, linewidth=2, edgecolor='red', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(left, top - 5, text, fontsize=8, color='blue', bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        os.remove(temp_image_path)\n",
    "    txt=\"\"\n",
    "    for i in extracted_text:\n",
    "        txt+=i\n",
    "        txt+='\\n'\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535e6d28-1fce-47cf-82b4-297de89cf5b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# In case any update in embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f12f2f9d-a43e-4072-9111-dbe0b5bad6cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Contractual Document Document\n",
      "Processing Regulatory and Compliance Sub Categories Document\n",
      "Processing IPR Categories Document\n",
      "Processing Litigation and Dispute Categories Document\n",
      "Processing Property and Real Estate Document\n",
      "Processing Corporate and Governance Categories Document\n",
      "Processing HR Document categories Document\n",
      "Processing Finance and Banking Categories Document\n",
      "Processing Government and public laws Document\n",
      "Processing Personal and Civil Legal Documents Document\n",
      "Processing Bill of Entry Document\n"
     ]
    }
   ],
   "source": [
    "paths=[\n",
    "    \"/home/pa2/ml/Document-Classification/CLASSIFICATION FILES/1.Contractual Document.txt\",\n",
    "    \"/home/pa2/ml/Document-Classification/CLASSIFICATION FILES/2.Regulatory and Compliance Sub Categories.txt\",\n",
    "    \"/home/pa2/ml/Document-Classification/CLASSIFICATION FILES/3.IPR Categories.txt\",\n",
    "    \"/home/pa2/ml/Document-Classification/CLASSIFICATION FILES/4.Litigation and Dispute Categories.txt\",\n",
    "    \"/home/pa2/ml/Document-Classification/CLASSIFICATION FILES/5.Property and Real Estate.txt\",\n",
    "    \"/home/pa2/ml/Document-Classification/CLASSIFICATION FILES/6.Corporate and Governance Categories.txt\",\n",
    "    \"/home/pa2/ml/Document-Classification/CLASSIFICATION FILES/7.HR Document categories.txt\",\n",
    "    \"/home/pa2/ml/Document-Classification/CLASSIFICATION FILES/8.Finance and Banking Categories.txt\",\n",
    "    \"/home/pa2/ml/Document-Classification/CLASSIFICATION FILES/9.Government and public laws.txt\",\n",
    "    \"/home/pa2/ml/Document-Classification/CLASSIFICATION FILES/10.Personal and Civil Legal Documents.txt\",\n",
    "    \"/home/pa2/ml/Document-Classification/CLASSIFICATION FILES/11.Bill of Entry.txt\"\n",
    "]\n",
    "emb_dict={}\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1500,\n",
    "        chunk_overlap=0\n",
    "    )\n",
    "model=HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    ")\n",
    "for i in paths:\n",
    "    name=i.split('/')[-1].split('.')[1]\n",
    "    print(\"Processing\",name,\"Document\")\n",
    "    with open(i,'r') as file:\n",
    "        new_context=file.read()\n",
    "    docs=text_splitter.split_text(new_context)\n",
    "    docs=model.embed_documents(docs)\n",
    "    doc=np.zeros(len(docs[0]))\n",
    "    for j in docs:\n",
    "        doc+=j\n",
    "    emb_dict[i]=doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa76d27f-6fbd-45f5-8342-2d465612a5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "[OK] Indexed 11 documents.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "CORE = \"emb_core\"\n",
    "SOLR_URL = f\"http://localhost:8983/solr/{CORE}/update?commit=true\"\n",
    "HEADERS  = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "def index_emb_dict(emb_dict):\n",
    "    docs = []\n",
    "    for path, arr in emb_dict.items():\n",
    "        vec = arr.tolist()\n",
    "        docs.append({\n",
    "            \"id\": path,           \n",
    "            \"filepath\": path,\n",
    "            \"total_embedding\": vec\n",
    "        })\n",
    "    print(len(docs))\n",
    "    resp = requests.post(SOLR_URL, headers=HEADERS, json=docs)\n",
    "    if resp.ok:\n",
    "        print(f\"[OK] Indexed {len(docs)} documents.\")\n",
    "    else:\n",
    "        print(f\"[FAIL] {resp.status_code}\\n{resp.text}\")\n",
    "index_emb_dict(emb_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7d322e-e3ab-46bb-96f1-4a54917854af",
   "metadata": {},
   "source": [
    "# continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64052477-8234-495a-be81-e0f05095398b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " emb_dict having 10 entries.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1500,\n",
    "        chunk_overlap=0\n",
    "    )\n",
    "model=HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    ")\n",
    "CORE     = \"emb_core\"\n",
    "BASE_URL = f\"http://localhost:8983/solr/{CORE}/select\"\n",
    "resp0 = requests.get(BASE_URL, params={\"q\": \"*:*\", \"wt\": \"json\", \"rows\": 0})\n",
    "resp0.raise_for_status()\n",
    "total = resp0.json()[\"response\"][\"numFound\"]\n",
    "resp1 = requests.get(\n",
    "    BASE_URL,\n",
    "    params={\"q\": \"*:*\", \"wt\": \"json\", \"rows\": total}\n",
    ")\n",
    "resp1.raise_for_status()\n",
    "docs = resp1.json()[\"response\"][\"docs\"]\n",
    "emb_dict = {}\n",
    "for d in docs:\n",
    "    path = d[\"id\"]\n",
    "    vec  = d.get(\"total_embedding\", [])\n",
    "    emb_dict[path] = np.array(vec, dtype=float)\n",
    "emb_dict.pop(\"/home/pa2/ml/Document-Classification/CLASSIFICATION FILES/11.Bill of Entry.txt\")\n",
    "print(f\" emb_dict having {len(emb_dict)} entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3223097-7bcd-4789-924f-69e9cdff919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(text,emb_dict,name):\n",
    "    docs1=text_splitter.split_text(text)\n",
    "    for i in range(len(docs1)):\n",
    "        docs1[i]+=\" \"+name\n",
    "    docs1=model.embed_documents(docs1)\n",
    "    doc1=np.zeros(len(docs1[0]))\n",
    "    for k in docs1:\n",
    "        doc1+=k\n",
    "    file=\"\"\n",
    "    csin1=0\n",
    "    for i in emb_dict:\n",
    "        name=i.split('/')[-1].split('.')[1]\n",
    "        # print(name)\n",
    "        csin=cosine_similarity([emb_dict[i]],[doc1])\n",
    "        print(name)\n",
    "        print(csin)\n",
    "        if csin>csin1:\n",
    "            csin1=csin\n",
    "            file=name\n",
    "    return file,docs1,doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06e2de20-61ae-407e-b2b9-e65a1b61aabb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 1024,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.2\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 1024,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): TrOCRScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 1024)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "detector = ocr_predictor(pretrained=True, assume_straight_pages=True).to(device)\n",
    "trocr_model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-handwritten\").to(device)\n",
    "trocr_processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-handwritten\")\n",
    "trocr_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c34be6bb-1f7b-4c6f-b719-57dd82726084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# below is function to send result after classification to solar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5f97e02-c58a-4ffe-a374-d8e7f97fd1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOLR_URL = \"http://localhost:8983/solr/mycore/update/json/docs?commit=true\"\n",
    "HEADERS  = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "def send_result_to_solr(result: dict):\n",
    "    \"\"\"\n",
    "    Sends one document (result dict) directly to Solr.\n",
    "    \"\"\"\n",
    "    for field in (\"section_embedding\", \"total_embedding\"):\n",
    "        val = result.get(field)\n",
    "        if hasattr(val, \"tolist\"):\n",
    "            result[field] = val.tolist()\n",
    "\n",
    "    resp = requests.post(SOLR_URL, headers=HEADERS, json=result)\n",
    "    if resp.status_code == 200:\n",
    "        print(f\"[OK] Indexed id={result.get('id')}\")\n",
    "    else:\n",
    "        print(f\"[FAIL] id={result.get('id')} → {resp.status_code}\\n{resp.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "627d1d3b-7917-4fc8-bbad-50aff944215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# below function will read all files from given folder , classify it and then store it into solar db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "212e1577-26cf-4b8e-9785-104276f71125",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SupplyOutward-MIPL-107-2025-26 Document is pdf of text\n",
      "Contractual Document\n",
      "[[0.90540444]]\n",
      "Regulatory and Compliance Sub Categories\n",
      "[[0.91168815]]\n",
      "IPR Categories\n",
      "[[0.91642317]]\n",
      "Litigation and Dispute Categories\n",
      "[[0.92016424]]\n",
      "Property and Real Estate\n",
      "[[0.91487161]]\n",
      "Corporate and Governance Categories\n",
      "[[0.90393604]]\n",
      "HR Document categories\n",
      "[[0.90602101]]\n",
      "Finance and Banking Categories\n",
      "[[0.91933481]]\n",
      "Government and public laws\n",
      "[[0.91541262]]\n",
      "Personal and Civil Legal Documents\n",
      "[[0.91217871]]\n",
      "Document best classified as Litigation and Dispute Categories\n"
     ]
    }
   ],
   "source": [
    "def main(folder_path=\"/home/pa2/Downloads/Marg Invoices/(8-4-2025)/\"):\n",
    "    paths=os.listdir(folder_path)\n",
    "    for path in paths:\n",
    "        if not path.lower().endswith(\".pdf\"):\n",
    "            continue \n",
    "        name=path.split(\"\\\\\")[-1].split('.')[0]\n",
    "        os.makedirs(\"/home/pa2/ml/tf_gpu/OCR/D:/BPL/BLS/Result/\", exist_ok=True)\n",
    "        output_json=f\"/home/pa2/ml/tf_gpu/OCR/D:/BPL/BLS/Result/{name}.json\"\n",
    "        ppath=os.path.join(folder_path,path)\n",
    "        # print(classify_and_extract_pdf(ppath))\n",
    "        text1=\"\"\n",
    "        results=[]\n",
    "        \n",
    "        text1+=name;\n",
    "        text1+=\" \"\n",
    "        out=classify_and_extract_pdf(ppath)\n",
    "        # print(out)\n",
    "        if(out==0):\n",
    "            print(f\"{name} Document seems to be a pdf of image\")\n",
    "            text1=process_pdf(ppath,trocr_model,detector)\n",
    "        else:\n",
    "            print(f\"{name} Document is pdf of text\")\n",
    "            text1=out\n",
    "        class1,docs1,doc1=classification(text1,emb_dict,name)\n",
    "        print(\"Document best classified as\",class1)\n",
    "        # print(text1)\n",
    "        break\n",
    "        result = {\n",
    "            \"id\":name,\n",
    "            \"class\": class1,\n",
    "            \"text\": text1, \n",
    "            \"section_embedding\": docs1, \n",
    "            \"total_embedding\": doc1.tolist() if hasattr(doc1, \"tolist\") else doc1 \n",
    "        }\n",
    "        send_result_to_solr(result)\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fb28144-7362-4d60-b96d-653109cb8b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1500,\n",
    "        chunk_overlap=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25367ed2-ac51-4ca3-a9f9-9346ceda28b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_conversion(li,embedding_dim=1024):\n",
    "    flat_embedding=li\n",
    "    if len(flat_embedding) % embedding_dim != 0:\n",
    "        raise ValueError(\"Flat embedding length is not divisible by embedding dimension.\")\n",
    "    \n",
    "    num_vectors = len(flat_embedding) // embedding_dim\n",
    "    section_embedding_2d = [\n",
    "        flat_embedding[i * embedding_dim: (i + 1) * embedding_dim]\n",
    "        for i in range(num_vectors)\n",
    "    ]\n",
    "    return section_embedding_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39309d52-e426-4cec-9998-f8c9be308c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# below cell is to get data from solar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d90111c-10dd-410a-8f77-2cd812b131f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_name = \"mycore\"\n",
    "solr_url = f\"http://localhost:8983/solr/{core_name}/select\"\n",
    "initial_params = {\n",
    "    \"q\": \"*:*\",\n",
    "    \"wt\": \"json\",\n",
    "    \"rows\": 0\n",
    "}\n",
    "response = requests.get(solr_url, params=initial_params)\n",
    "total_docs = response.json()[\"response\"][\"numFound\"]\n",
    "params = {\n",
    "    \"q\": \"*:*\",\n",
    "    \"wt\": \"json\",\n",
    "    \"rows\": total_docs\n",
    "}\n",
    "response = requests.get(solr_url, params=params)\n",
    "docs = response.json()[\"response\"][\"docs\"]\n",
    "dic={}\n",
    "docs1=docs\n",
    "for i in range(len(docs)):\n",
    "    li=[docs[i]['class'][0],docs[i]['text'][0],embedding_conversion(docs[i][\"section_embedding\"]),docs[i]['total_embedding']]\n",
    "    dic[i]=li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7754a907-bdd6-4f7b-8e46-182c4884c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from sklearn.metrics.pairwise import euclidean_distances,cosine_similarity\n",
    "model=HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    ")\n",
    "def context_provider(dic,query):\n",
    "    idx=''\n",
    "    query=model.embed_query(query)\n",
    "    query_emb=np.array([query],dtype='float32')\n",
    "    sim=0\n",
    "    for i in dic:\n",
    "        tot_emb=np.array([dic[i][-1]])\n",
    "\n",
    "        cos_sim = cosine_similarity(query_emb, tot_emb)\n",
    "        print(cos_sim)\n",
    "        if(cos_sim>sim):\n",
    "            sim=cos_sim\n",
    "            idx=i\n",
    "    text,sec_emb=dic[idx][1],np.array(dic[idx][2],dtype='float32')\n",
    "    faiss.normalize_L2(sec_emb)\n",
    "    db = faiss.IndexFlatIP(sec_emb.shape[1])\n",
    "    db.add(sec_emb)\n",
    "    D, I = db.search(query_emb, k=8)\n",
    "    context=\"\"\n",
    "    text_split=text_splitter.split_text(text)\n",
    "    # print(I)\n",
    "    for i in I[0]:\n",
    "        context+=text_split[i]\n",
    "        context+='\\n'\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7247c24b-5862-4ae7-9f4a-41a2727f0862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is your question::  what was reason for conflict between state and surendrasingh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73073701]]\n",
      "[[0.72662516]]\n",
      "[[0.82558457]]\n",
      "the case rests on circumstantial evidence has no role to play. It was observed in paragraph 40 as\n",
      "under:-\n",
      "Anil Surendrasingh Yadav vs State Of Gujarat on 27 December, 2019\n",
      "Indian Kanoon - http://indiankanoon.org/doc/137212109/\n",
      "25\n",
      "Indian Kanoon - http://indiankanoon.org/doc/137212109/\n",
      "9\n",
      "--- Page 1 ---\n",
      "Anil Surendrasingh Yadav vs State Of Gujarat on 27 December,\n",
      "2019\n",
      "Equivalent citations: AIRONLINE 2019 GUJ 853\n",
      "Author: Bela M. Trivedi\n",
      "Bench: Bela M. Trivedi, A.C. Rao\n",
      "       R/CR.A/1973/2019                                        CAV JUDGMENT\n",
      "            IN THE HIGH COURT OF GUJARAT AT AHMEDABAD\n",
      "                     R/CRIMINAL APPEAL NO. 1973 of 2019\n",
      "                                With\n",
      "              R/CRIMINAL CONFIRMATION CASE NO. 2 of 2019\n",
      "FOR APPROVAL AND SIGNATURE:\n",
      "HONOURABLE MS.JUSTICE BELA M. TRIVEDI                           Sd/-\n",
      "and\n",
      "HONOURABLE MR.JUSTICE A.C. RAO                                  Sd/-\n",
      "==========================================================\n",
      "1     Whether Reporters of Local Papers may be allowed to\n",
      "      see the judgment ?                                                   YES\n",
      "2     To be referred to the Reporter or not ?\n",
      "                                                                           YES\n",
      "3     Whether their Lordships wish to see the fair copy of the\n",
      "      judgment ?                                                           NO\n",
      "4     Whether this case involves a substantial question of law\n",
      "      as to the interpretation of the Constitution of India or any         NO\n",
      "      order made thereunder ?\n",
      "==========================================================\n",
      "                          ANIL SURENDRASINGH YADAV\n",
      "                                     Versus\n",
      "                               STATE OF GUJARAT\n",
      "other party to cross−examine him. However, if the statements made are false, the\n",
      "Court is entitled to draw adverse inferences and pass consequential orders, as may be\n",
      "called for, in accordance with law. The primary purpose is to establish a direct\n",
      "dialogue between the Court and the accused and to put every important\n",
      "incriminating piece of evidence to the accused and grant him an opportunity to\n",
      "Anil Surendrasingh Yadav vs State Of Gujarat on 27 December, 2019\n",
      "Indian Kanoon - http://indiankanoon.org/doc/137212109/\n",
      "20\n",
      "when the alternative of life-imprisonment is totally inadequate, and therefore,\n",
      "unquestionably foreclosed. According to Mr.Vyas, the Special Court had committed\n",
      "gross error in not following the guidelines issued by the Supreme Court in case of\n",
      "Bachan Singh v. State of Punjab and in the case of Machhi Singh and Ors. v. State of\n",
      "Punjab (supra) by not comparing the mitigating circumstances with the aggravating\n",
      "circumstances. He also submitted that the Special Court ought to have considered the\n",
      "age of the accused and the probability that the accused can be reformed and\n",
      "rehabilitated.\n",
      "Anil Surendrasingh Yadav vs State Of Gujarat on 27 December, 2019\n",
      "Indian Kanoon - http://indiankanoon.org/doc/137212109/\n",
      "21\n",
      "Anil Surendrasingh Yadav vs State Of Gujarat on 27 December, 2019\n",
      "Indian Kanoon - http://indiankanoon.org/doc/137212109/\n",
      "3\n",
      "of Objects and Reasons of amendment, Parliament has shown its concern of the fact\n",
      "that \"in recent past incidents of child sexual abuse cases administering the inhuman\n",
      "Anil Surendrasingh Yadav vs State Of Gujarat on 27 December, 2019\n",
      "Indian Kanoon - http://indiankanoon.org/doc/137212109/\n",
      "27\n",
      "seem to proceed upon the view that confessions made by an accused to a police\n",
      "officer or made by him while he is in the custody of a police officer are not to be\n",
      "trusted, and should not be used in evidence against him. They are based upon\n",
      "grounds of public policy, and the fullest effect should be given to them.\"\n",
      "12. So far as the facts of the present case are concerned, the extra-judicial confession made by the\n",
      "accused before the doctor, while he was in police custody, would therefore be hit by Section 26 of the\n",
      "Evidence Act and could not be read in evidence. It is also required to be noted that in response to\n",
      "the suggestion put by the learned Advocate for the accused in the cross-examination, the PW-1\n",
      "Dr.Piyush Chunilal Vasava had admitted the presence of the police officer when the accused was\n",
      "being examined by him. Of course, he had denied that the accused had not given any such history as\n",
      "recorded in the case paper. It is also true that the Medical Officer being an independent witness, he\n",
      "had no reason to falsely implicate the accused by recording such history in the case paper,\n",
      "nonetheless, even if it is presumed that the said history was given by the accused, the same having\n",
      "been given by him while in police custody and in the presence of the police officer, such admission\n",
      "or confession made by him would be inadmissible in evidence by virtue of Section 26 of the\n",
      "Evidence Act.\n",
      "Other Evidence led by the Prosecution:-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query=input(\"What is your question:: \")\n",
    "print(context_provider(dic,query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecefd5f-593b-42bb-ac45-630337d95795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
